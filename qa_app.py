"""
Streamlit application to explore and query speeches by IMF Managing Director
Kristalina Georgieva.  The application loads a pre‑built dataset of
speeches (generated by ``extract_imf_speeches.py``) and allows users
to search for topics across the transcripts, filter by themes, and
view speeches chronologically.  The app organises results by date and
provides contextual snippets for each speech that match the query.

To run the app locally:

    streamlit run qa_app.py

Make sure ``speeches_data.pkl`` (produced by ``extract_imf_speeches.py``)
is present in the same directory or in ``/home/oai/share``.
"""

import datetime
import os
import re
from typing import List, Tuple

import pandas as pd
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel


@st.cache_data(show_spinner=False)
def load_data(pkl_path: str) -> pd.DataFrame:
    """Load the speeches DataFrame from the pickle file.

    The DataFrame includes columns: title, date, link, location,
    themes (list), and transcript.  The date column is converted to
    datetime for easy sorting and filtering.
    """
    df = pd.read_pickle(pkl_path)
    # Ensure date is datetime
    df['date'] = pd.to_datetime(df['date'])
    return df


@st.cache_data(show_spinner=False)
def build_tfidf_matrix(texts: List[str]) -> Tuple[TfidfVectorizer, any]:
    """Compute the TF‑IDF matrix for a list of documents.

    Returns the vectoriser and the matrix.  Caching avoids recomputation
    across Streamlit reruns.
    """
    vectoriser = TfidfVectorizer(stop_words='english')
    matrix = vectoriser.fit_transform(texts)
    return vectoriser, matrix


def search_speeches(query: str, df: pd.DataFrame, vectoriser: TfidfVectorizer, matrix) -> pd.DataFrame:
    """Perform a TF‑IDF search over the speech transcripts.

    Returns a DataFrame of speeches ordered by descending relevance to
    the query.  Each row includes a ``score`` column with the cosine
    similarity value.
    """
    if not query.strip():
        return pd.DataFrame()
    query_vec = vectoriser.transform([query])
    scores = linear_kernel(query_vec, matrix).flatten()
    df_results = df.copy().assign(score=scores)
    df_results = df_results[df_results['score'] > 0].sort_values(by=['score', 'date'], ascending=[False, False])
    return df_results


def highlight_matches(text: str, query: str, max_sentences: int = 3) -> str:
    """Extract and highlight sentences containing query terms.

    Returns a snippet of up to ``max_sentences`` sentences with the
    search terms highlighted using Markdown bold.  If no sentence
    matches, returns the first 250 characters of the text.
    """
    # Normalise query: split into words, ignore case
    words = [re.escape(w) for w in query.strip().split() if w]
    if not words:
        return text[:250]
    pattern = re.compile(r'(' + '|'.join(words) + r')', re.IGNORECASE)
    # Split transcript into sentences by simple punctuation
    sentences = re.split(r'(?<=[.!?])\s+', text)
    matched = []
    for s in sentences:
        if pattern.search(s):
            # Highlight all occurrences
            highlighted = pattern.sub(lambda m: f"**{m.group(0)}**", s)
            matched.append(highlighted.strip())
            if len(matched) >= max_sentences:
                break
    if matched:
        return ' '.join(matched)
    else:
        # Return truncated snippet
        snippet = sentences[0] if sentences else text
        return snippet[:250] + ('…' if len(snippet) > 250 else '')


def summarise_text(text: str, query: str, max_sentences: int = 5) -> str:
    """Generate a simple summary of ``text`` focused on ``query`` terms.

    The function extracts sentences containing any of the query words and
    returns up to ``max_sentences`` of them joined together.  If no
    sentences match, it returns the first 500 characters of the text.

    This heuristic summariser provides a lightweight alternative to
    advanced NLP summarisation libraries, which may not be available in
    all environments.
    """
    words = [re.escape(w) for w in query.strip().split() if w]
    if not words:
        return text[:500] + ('…' if len(text) > 500 else '')
    pattern = re.compile(r'(' + '|'.join(words) + r')', re.IGNORECASE)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    matched = []
    for s in sentences:
        if pattern.search(s):
            matched.append(s.strip())
            if len(matched) >= max_sentences:
                break
    if matched:
        return ' '.join(matched)
    else:
        return text[:500] + ('…' if len(text) > 500 else '')


def extract_keywords(text: str, n: int = 5) -> List[str]:
    """Extract the top ``n`` keywords from ``text`` based on term frequency.

    This utility tokenises the text, filters out common stop words and
    non‑alphabetic tokens, and returns the most frequent words.  It
    serves as a simple proxy for identifying the main topics discussed
    within a body of text.
    """
    # Minimal stop words list.  For a more comprehensive list consider
    # importing from nltk or another library.
    stop_words = set(
        [
            'the', 'and', 'to', 'of', 'a', 'in', 'for', 'that', 'on',
            'is', 'with', 'as', 'are', 'be', 'this', 'by', 'an', 'it',
            'or', 'from', 'at', 'our', 'we', 'will', 'has', 'have',
            'not', 'new', 'more', 'can'
        ]
    )
    words = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())
    freq = {}
    for w in words:
        if w in stop_words:
            continue
        freq[w] = freq.get(w, 0) + 1
    # Sort by frequency descending
    sorted_words = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [w for w, _ in sorted_words[:n]]


def bullet_summary(text: str, n: int = 3) -> List[str]:
    """Create a simple bullet‑point summary of the given speech text.

    The function splits the text into sentences and selects the first
    ``n`` sentences that are reasonably short (<= 200 characters).  It
    serves as a heuristic to extract the main introductory points of a
    speech for quick reading.
    """
    sentences = re.split(r'(?<=[.!?])\s+', text)
    bullets = []
    for s in sentences:
        s = s.strip()
        if s:
            bullets.append(s)
            if len(bullets) >= n:
                break
    return bullets


def top_quotes_across_results(query: str, transcripts: List[str], k: int = 3) -> List[str]:
    """Return the top ``k`` sentences containing the query terms across multiple transcripts.

    This function searches through all transcripts, extracts sentences
    containing any of the query words, and sorts them by length (shorter
    first) to provide concise quotes.  Duplicate sentences are
    removed.  If fewer than ``k`` sentences are found, the remaining
    slots are left empty.
    """
    words = [re.escape(w) for w in query.strip().split() if w]
    pattern = re.compile(r'(' + '|'.join(words) + r')', re.IGNORECASE)
    found = []
    for text in transcripts:
        sentences = re.split(r'(?<=[.!?])\s+', text)
        for s in sentences:
            if pattern.search(s):
                quote = s.strip()
                if quote not in found:
                    found.append(quote)
    # Sort by length for concise quotes
    found = sorted(found, key=lambda s: len(s))
    return found[:k]


def main():
    st.set_page_config(page_title="IMF MD Speeches Explorer", layout="wide")
    st.title("Kristalina Georgieva Speeches (Aug 2022 – Aug 2025)")
    st.write(
        "Use this tool to explore the speeches delivered by IMF Managing Director "
        "Kristalina Georgieva between **August 7 2022** and **August 7 2025**. "
        "You can search for topics within speech transcripts, filter by themes, location and date, "
        "view trends over time, and export results."
    )
    # Locate the pickle file.  Look in the current directory first, then fallback to /home/oai/share.
    pkl_candidates = [
        os.path.join(os.getcwd(), 'speeches_data.pkl'),
        os.path.join('/home/oai/share', 'speeches_data.pkl'),
    ]
    pkl_path = None
    for p in pkl_candidates:
        if os.path.exists(p):
            pkl_path = p
            break
    if not pkl_path:
        st.error("Could not find speeches_data.pkl. Please run the extraction script first.")
        return
    df = load_data(pkl_path)
    # Build TF‑IDF matrix for searching
    vectoriser, matrix = build_tfidf_matrix(df['transcript'].tolist())
    # Sidebar filters
    st.sidebar.header("Filters")
    # Theme filter sidebar
    all_themes = sorted({theme for sublist in df['themes'] for theme in sublist})
    selected_themes = st.sidebar.multiselect(
        'By Theme', options=all_themes, default=[]
    )
    # Location filter sidebar
    unique_locations = sorted({loc for loc in df['location'] if loc})
    selected_locations = st.sidebar.multiselect(
        'By Location', options=unique_locations, default=[]
    )
    # Date range filter sidebar
    min_date, max_date = df['date'].min().date(), df['date'].max().date()
    date_range = st.sidebar.date_input(
        'By Date Range', value=(min_date, max_date), min_value=min_date, max_value=max_date
    )
    # Display dataset overview charts
    st.subheader("Dataset Overview")
    col1, col2 = st.columns(2)
    with col1:
        # Theme distribution bar chart
        theme_counts = pd.Series([t for sub in df['themes'] for t in sub]).value_counts().reset_index()
        theme_counts.columns = ['Theme', 'Count']
        st.markdown("**Speeches per Theme**")
        st.bar_chart(theme_counts.set_index('Theme'))
    with col2:
        # Timeline of number of speeches per year
        df['year'] = df['date'].dt.year
        year_counts = df.groupby('year').size().reset_index(name='Count')
        st.markdown("**Speeches per Year**")
        st.line_chart(year_counts.set_index('year'))
    # Keyword overview across all speeches
    overall_keywords = extract_keywords(' '.join(df['transcript'].tolist()), n=10)
    st.markdown("**Top Keywords Across All Speeches:** " + ', '.join(overall_keywords))
    # Search box
    query = st.text_input('Enter a topic or keyword to search the speeches')
    if query:
        results = search_speeches(query, df, vectoriser, matrix)
        # Apply theme filter
        if selected_themes:
            results = results[results['themes'].apply(lambda lst: any(t in selected_themes for t in lst))]
        # Apply location filter
        if selected_locations:
            results = results[results['location'].isin(selected_locations)]
        # Apply date range filter
        if isinstance(date_range, tuple) and len(date_range) == 2:
            start_date, end_date = date_range
            results = results[(results['date'].dt.date >= start_date) & (results['date'].dt.date <= end_date)]
        # Sort results by recency (most recent first) to prioritise the latest speeches
        results = results.sort_values(by='date', ascending=False)
        # Show results count and export option
        st.write(f"Found **{len(results)}** speeches matching your query.")
        if not results.empty:
            # Export results to CSV button
            csv = results[['title', 'date', 'link', 'location', 'themes']].to_csv(index=False)
            st.download_button(
                label="Download results as CSV", data=csv, file_name="search_results.csv", mime="text/csv"
            )
            # Top quotes across results
            quotes = top_quotes_across_results(query, results['transcript'].tolist(), k=3)
            if quotes:
                st.markdown("**Top Quotes Containing Your Keyword:**")
                for i, q in enumerate(quotes, 1):
                    st.markdown(f"- {q}")
        # Display individual results
        for _, row in results.iterrows():
            with st.expander(f"{row['date'].strftime('%Y-%m-%d')} – {row['title']}"):
                st.markdown(f"**Location:** {row['location'] or 'N/A'}  ")
                st.markdown(f"**Themes:** {', '.join(row['themes']) if row['themes'] else 'N/A'}  ")
                st.markdown(f"**Link:** [View Speech]({row['link']})  ")
                # Bullet summary
                bullets = bullet_summary(row['transcript'], n=3)
                st.markdown("**Key Points:**")
                for b in bullets:
                    st.markdown(f"- {b}")
                # Show snippet around query
                snippet = highlight_matches(row['transcript'], query)
                st.markdown("**Snippet:**")
                st.markdown(snippet)
                # Optionally show full transcript
                if st.checkbox(f"Show full transcript of '{row['title']}'", key=row['link']):
                    st.text(row['transcript'])
    else:
        st.write("Enter a keyword above to start searching.")


if __name__ == '__main__':
    main()