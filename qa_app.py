"""
Streamlit application to explore and query speeches by IMF Managing Director
Kristalina Georgieva.  The application loads a pre‑built dataset of
speeches (generated by ``extract_imf_speeches.py``) and allows users
to search for topics across the transcripts, filter by themes, and
view speeches chronologically.  The app organises results by date and
provides contextual snippets for each speech that match the query.

To run the app locally:

    streamlit run qa_app.py

Make sure ``speeches_data.pkl`` (produced by ``extract_imf_speeches.py``)
is present in the same directory or in ``/home/oai/share``.
"""

import datetime
import os
import re
from typing import List, Tuple

import pandas as pd
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel


@st.cache_data(show_spinner=False)
def load_data(pkl_path: str) -> pd.DataFrame:
    """Load the speeches DataFrame from the pickle file.

    The DataFrame includes columns: title, date, link, location,
    themes (list), and transcript.  The date column is converted to
    datetime for easy sorting and filtering.
    """
    df = pd.read_pickle(pkl_path)
    # Ensure date is datetime
    df['date'] = pd.to_datetime(df['date'])
    return df


@st.cache_data(show_spinner=False)
def build_tfidf_matrix(texts: List[str]) -> Tuple[TfidfVectorizer, any]:
    """Compute the TF‑IDF matrix for a list of documents.

    Returns the vectoriser and the matrix.  Caching avoids recomputation
    across Streamlit reruns.
    """
    vectoriser = TfidfVectorizer(stop_words='english')
    matrix = vectoriser.fit_transform(texts)
    return vectoriser, matrix


def search_speeches(query: str, df: pd.DataFrame, vectoriser: TfidfVectorizer, matrix) -> pd.DataFrame:
    """Perform a TF‑IDF search over the speech transcripts.

    Returns a DataFrame of speeches ordered by descending relevance to
    the query.  Each row includes a ``score`` column with the cosine
    similarity value.
    """
    if not query.strip():
        return pd.DataFrame()
    query_vec = vectoriser.transform([query])
    scores = linear_kernel(query_vec, matrix).flatten()
    df_results = df.copy().assign(score=scores)
    df_results = df_results[df_results['score'] > 0].sort_values(by=['score', 'date'], ascending=[False, False])
    return df_results


def highlight_matches(text: str, query: str, max_sentences: int = 3) -> str:
    """Extract and highlight sentences containing query terms.

    Returns a snippet of up to ``max_sentences`` sentences with the
    search terms highlighted using Markdown bold.  If no sentence
    matches, returns the first 250 characters of the text.
    """
    # Normalise query: split into words, ignore case
    words = [re.escape(w) for w in query.strip().split() if w]
    if not words:
        return text[:250]
    pattern = re.compile(r'(' + '|'.join(words) + r')', re.IGNORECASE)
    # Split transcript into sentences by simple punctuation
    sentences = re.split(r'(?<=[.!?])\s+', text)
    matched = []
    for s in sentences:
        if pattern.search(s):
            # Highlight all occurrences
            highlighted = pattern.sub(lambda m: f"**{m.group(0)}**", s)
            matched.append(highlighted.strip())
            if len(matched) >= max_sentences:
                break
    if matched:
        return ' '.join(matched)
    else:
        # Return truncated snippet
        snippet = sentences[0] if sentences else text
        return snippet[:250] + ('…' if len(snippet) > 250 else '')


def summarise_text(text: str, query: str, max_sentences: int = 5) -> str:
    """Generate a simple summary of ``text`` focused on ``query`` terms.

    The function extracts sentences containing any of the query words and
    returns up to ``max_sentences`` of them joined together.  If no
    sentences match, it returns the first 500 characters of the text.

    This heuristic summariser provides a lightweight alternative to
    advanced NLP summarisation libraries, which may not be available in
    all environments.
    """
    words = [re.escape(w) for w in query.strip().split() if w]
    if not words:
        return text[:500] + ('…' if len(text) > 500 else '')
    pattern = re.compile(r'(' + '|'.join(words) + r')', re.IGNORECASE)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    matched = []
    for s in sentences:
        if pattern.search(s):
            matched.append(s.strip())
            if len(matched) >= max_sentences:
                break
    if matched:
        return ' '.join(matched)
    else:
        return text[:500] + ('…' if len(text) > 500 else '')


def extract_keywords(text: str, n: int = 5) -> List[str]:
    """Extract the top ``n`` keywords from ``text`` based on term frequency.

    This utility tokenises the text, filters out common stop words and
    non‑alphabetic tokens, and returns the most frequent words.  It
    serves as a simple proxy for identifying the main topics discussed
    within a body of text.
    """
    # Minimal stop words list.  For a more comprehensive list consider
    # importing from nltk or another library.
    stop_words = set(
        [
            'the', 'and', 'to', 'of', 'a', 'in', 'for', 'that', 'on',
            'is', 'with', 'as', 'are', 'be', 'this', 'by', 'an', 'it',
            'or', 'from', 'at', 'our', 'we', 'will', 'has', 'have',
            'not', 'new', 'more', 'can'
        ]
    )
    words = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())
    freq = {}
    for w in words:
        if w in stop_words:
            continue
        freq[w] = freq.get(w, 0) + 1
    # Sort by frequency descending
    sorted_words = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [w for w, _ in sorted_words[:n]]


def main():
    st.set_page_config(page_title="IMF MD Speeches Explorer", layout="wide")
    st.title("Kristalina Georgieva Speeches (Aug 2022 – Aug 2025)")
    st.write(
        "Use this tool to explore the speeches delivered by IMF Managing Director "
        "Kristalina Georgieva between **August 7 2022** and **August 7 2025**. "
        "Enter a topic or keyword to search across the speech transcripts. "
        "Results are ordered by relevance and recency."
    )
    # Locate the pickle file.  Look in the current directory first, then
    # fallback to /home/oai/share.
    pkl_candidates = [
        os.path.join(os.getcwd(), 'speeches_data.pkl'),
        os.path.join('/home/oai/share', 'speeches_data.pkl'),
    ]
    pkl_path = None
    for p in pkl_candidates:
        if os.path.exists(p):
            pkl_path = p
            break
    if not pkl_path:
        st.error("Could not find speeches_data.pkl. Please run the extraction script first.")
        return
    df = load_data(pkl_path)
    # Build TF‑IDF matrix
    vectoriser, matrix = build_tfidf_matrix(df['transcript'].tolist())
    # Theme filter sidebar
    all_themes = sorted({theme for sublist in df['themes'] for theme in sublist})
    selected_themes = st.sidebar.multiselect(
        'Filter by Theme (optional)', options=all_themes, default=[]
    )
    # Date range filter sidebar
    min_date, max_date = df['date'].min().date(), df['date'].max().date()
    date_range = st.sidebar.date_input(
        'Filter by Date Range', value=(min_date, max_date), min_value=min_date, max_value=max_date
    )
    query = st.text_input('Enter a topic or keyword')
    if query:
        results = search_speeches(query, df, vectoriser, matrix)
        # Apply theme filter
        if selected_themes:
            mask = results['themes'].apply(lambda lst: any(t in selected_themes for t in lst))
            results = results[mask]
        # Apply date range filter
        if isinstance(date_range, tuple) and len(date_range) == 2:
            start_date, end_date = date_range
            results = results[(results['date'].dt.date >= start_date) & (results['date'].dt.date <= end_date)]
        # Sorting option
        sort_option = st.sidebar.selectbox(
            'Sort results by', options=['Relevance', 'Recency'], index=0
        )
        if sort_option == 'Recency':
            results = results.sort_values(by='date', ascending=False)
        st.write(f"Found **{len(results)}** speeches matching your query.")
        if not results.empty:
            # High level summary across all results
            with st.expander('Summary across all matching speeches'):
                all_text = ' '.join(results['transcript'].tolist())
                summary = summarise_text(all_text, query, max_sentences=5)
                st.markdown(summary)
                # Keyword extraction and year breakdown
                st.subheader('Key statistics by year')
                year_info = []
                for year, group in results.groupby(results['date'].dt.year):
                    year_text = ' '.join(group['transcript'].tolist())
                    year_summary = summarise_text(year_text, query, max_sentences=3)
                    keywords = extract_keywords(year_text, n=5)
                    year_info.append(
                        {
                            'Year': year,
                            'Speeches': len(group),
                            'Top words': ', '.join(keywords),
                            'Summary': year_summary,
                        }
                    )
                # Display year breakdown
                for info in sorted(year_info, key=lambda x: x['Year'], reverse=True):
                    st.markdown(f"**{info['Year']}** – {info['Speeches']} speeches. Top words: {info['Top words']}")
                    st.markdown(info['Summary'])
                    st.markdown('')
        # Display individual results as an expandable list
        for _, row in results.iterrows():
            with st.expander(f"{row['date'].strftime('%Y-%m-%d')} – {row['title']}"):
                st.markdown(f"**Location:** {row['location'] or 'N/A'}  ")
                st.markdown(f"**Themes:** {', '.join(row['themes']) if row['themes'] else 'N/A'}  ")
                st.markdown(f"**Link:** [View Speech]({row['link']})  ")
                snippet = highlight_matches(row['transcript'], query)
                st.markdown(snippet)
                # Provide a brief summary for each speech
                st.markdown('**Summary:**')
                speech_summary = summarise_text(row['transcript'], query, max_sentences=5)
                st.markdown(speech_summary)
    else:
        st.write("Enter a keyword above to start searching.")


if __name__ == '__main__':
    main()